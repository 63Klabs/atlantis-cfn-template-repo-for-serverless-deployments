version: 0.2

# Recommended Pipeline Template "template-pipeline-two-stage" available from:
# s3://63klabz/atlantis/templates/v2/pipeline/template-pipeline-two-stage.yml
# https://github.com/63Klabs/atlantis-cfn-template-repo-for-serverless-deployments/blob/main/templates/v2/pipeline/template-pipeline-two-stage.yml


# Note: This is made for a two instance environment (BETA and PROD)
# Leave HOST buckets as ${HOST_BUCKET} if using the two stage pipeline above or if your deployment has a HOST_BUCKET variable.
# Change _PROD and _BETA ONLY if you you not have a HOST_BUCKET environment variable.
# Again, if using template-pipeline-two-stage for your pipeline you DO have a HOST_BUCKET env variable.

env:
  variables:
    SOURCE_DIR: "templates"
    S3_HOST_BASE_PATH: "atlantis"
    S3_HOST_BUCKET_PROD: "${HOST_BUCKET}" # Production bucket
    S3_HOST_BUCKET_BETA: "${HOST_BUCKET}" # Beta bucket
    S3_HOST_BUCKET: "${HOST_BUCKET}" # Default to production bucket
    DRYRUN: "" # set to "--dryrun" if performing dry runs

phases:
  install:
    runtime-versions:
      nodejs: latest
    commands:
      - pip install --upgrade awscli boto3
      - chmod +x ./scripts/*.{sh,py}

  pre_build:
    commands:
      # Set bucket based on deployment environment in a single command
      - export S3_HOST_BUCKET=$([ "$DEPLOY_ENV" != "PROD" ] && echo "$S3_HOST_BUCKET_BETA" || echo "$S3_HOST_BUCKET_PROD")
      - echo $S3_HOST_BUCKET/$S3_HOST_BASE_PATH
      - echo $DRYRUN

  build:
    commands:
      - echo "Build phase started at $(date)"

  post_build:
    commands:
      # We use versioning on the buckets so that we can specify a specific version of the template to use
      # Therefore, so we do not create extra versions, we will use only copy changed files.

      # Copy template files to host bucket
      - echo "Executing script to sync templates..."
      - ./scripts/sync_templates.sh $SOURCE_DIR $S3_HOST_BUCKET $S3_HOST_BASE_PATH/templates $DRYRUN
      # If you have additional directories you can sync them next as long as you have a unique local and unique remote directory (don't use same as previous othewise it will perform a delete)
#      - ./scripts/sync_templates.sh custom-templates $S3_HOST_BUCKET $S3_HOST_BASE_PATH/custom-templates $DRYRUN

      # Inventory host bucket templates
      - echo "Executing script to inventory S3..."
      - ./scripts/s3_inventory.py $S3_HOST_BUCKET $S3_HOST_BASE_PATH/templates --output-dir outputs
      - |
        if [ -z "$DRYRUN" ]; then
          aws s3 cp ./outputs/inventory_atlantis_templates.json s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/templates/inventory.json
          aws s3 cp ./outputs/inventory_atlantis_templates.txt s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/templates/inventory.txt
        else
          echo "[DRYRUN] Would copy inventory files to s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/templates/"
        fi

      # Send sharable scripts to host bucket
      - echo "Executing script to upload scripts..."
      - ./scripts/upload_scripts.sh $S3_HOST_BUCKET $S3_HOST_BASE_PATH/utilities scripts $DRYRUN

      # Inventory host bucket utilities
      - echo "Executing script to inventory S3..."
      - ./scripts/s3_inventory.py $S3_HOST_BUCKET $S3_HOST_BASE_PATH/utilities --output-dir outputs
      - |
        if [ -z "$DRYRUN" ]; then
          aws s3 cp ./outputs/inventory_atlantis_utilities.json s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/utilities/inventory.json
          aws s3 cp ./outputs/inventory_atlantis_utilities.txt s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/utilities/inventory.txt
        else
          echo "[DRYRUN] Would copy inventory files to s3://$S3_HOST_BUCKET/$S3_HOST_BASE_PATH/utilities/"
        fi

artifacts:
  files:
    - '**/*'