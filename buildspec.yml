version: 0.2

env:
  variables:
    SOURCE_DIR: "templates"
    S3_BASE_PATH: "atlantis"

    DRYRUN: "" # set to "--dryrun" if performing dry runs

phases:
  install:
    runtime-versions:
      nodejs: latest
    commands:
      - pwd
      - pip install --upgrade awscli boto3

      - chmod +x ./scripts/*.sh
      - chmod +x ./scripts/*.py

  pre_build:
    commands:

      - echo $HOST_BUCKET/$S3_BASE_PATH
      - echo $DRYRUN

  build:
    commands:

  post_build:
    commands:

      - pwd

      # We use versioning on the buckets so that we can specify a specific version of the template to use
      # Therefore, so we do not create extra versions, we will use only copy changed files.

      # Copy template files to host bucket
      - echo "Executing script to sync templates..."
      - ./scripts/sync_templates.sh $SOURCE_DIR $HOST_BUCKET $S3_BASE_PATH/templates $DRYRUN
      # If you have additional directories you can sync them next as long as you have a unique local and unique remote directory (don't use same as previous othewise it will perform a delete)
#      - ./scripts/sync_templates.sh custom-templates $HOST_BUCKET $S3_BASE_PATH/custom-templates $DRYRUN

      # Inventory host bucket templates
      - echo "Executing script to inventory S3..."
      - ./scripts/s3_inventory.py $HOST_BUCKET $S3_BASE_PATH/templates --output-dir outputs
      - [ -z "$DRYRUN" ] && aws s3 cp ./outputs/inventory_atlantis_templates.json s3://$HOST_BUCKET/$S3_BASE_PATH/templates/inventory.json || echo "[DRYRUN] Would copy ./outputs/inventory_atlantis_templates.json to s3://$HOST_BUCKET/$S3_BASE_PATH/templates"
      - [ -z "$DRYRUN" ] && aws s3 cp ./outputs/inventory_atlantis_templates.txt s3://$HOST_BUCKET/$S3_BASE_PATH/templates/inventory.txt || echo "[DRYRUN] Would copy ./outputs/inventory_atlantis_templates.txt to s3://$HOST_BUCKET/$S3_BASE_PATH/templates"

      # Send sharable scripts to host bucket
      - echo "Executing script to upload scripts..."
      - ./scripts/upload_scripts.sh $HOST_BUCKET $S3_BASE_PATH/utilities scripts $DRYRUN

      # Inventory host bucket utilities
      - echo "Executing script to inventory S3..."
      - ./scripts/s3_inventory.py $HOST_BUCKET $S3_BASE_PATH/utilities --output-dir outputs
      - [ -z "$DRYRUN" ] && aws s3 cp ./outputs/inventory_atlantis_utilities.json s3://$HOST_BUCKET/$S3_BASE_PATH/utilities/inventory.json || echo "[DRYRUN] Would copy ./outputs/inventory_atlantis_utilities.json to s3://$HOST_BUCKET/$S3_BASE_PATH/utilities"
      - [ -z "$DRYRUN" ] && aws s3 cp ./outputs/inventory_atlantis_utilities.txt s3://$HOST_BUCKET/$S3_BASE_PATH/utilities/inventory.txt || echo "[DRYRUN] Would copy ./outputs/inventory_atlantis_utilities.txt to s3://$HOST_BUCKET/$S3_BASE_PATH/utilities"

artifacts:
  files:
    - '**/*'